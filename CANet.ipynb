{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CANet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVN655AIMH76"
      },
      "source": [
        "Code for  model training was the result of the following paper:\r\n",
        "\r\n",
        "CANet: Cross-Disease Attention Network for Joint Diabetic Retinopathy and Diabetic Macular Edema Grading\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNht16J4Pa2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52159a1-892e-4d2b-c2ac-118da432095c"
      },
      "source": [
        "import gc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDdzPAolUVax"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJAx2GY2UKXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03111aff-1019-4e9c-a99c-6e60304bfa55"
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import tensorflow\n",
        "import tensorboard\n",
        "import random\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import scipy.misc\n",
        "from skimage.transform import resize\n",
        "#imports from drive\n",
        "# sys.path.append('/content/drive/MyDrive/lr_scheduler.py')  \n",
        "%cd '/content/drive/MyDrive'\n",
        "import lr_scheduler as l_rate\n",
        "import missidor\n",
        "# import average_result\n",
        "import resnet50\n",
        "# !nvidia-smi\n",
        "# print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg3C5M2Vf11f"
      },
      "source": [
        "File write \r\n",
        "\r\n",
        "writes the dataset image names to a file so run only once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i39y30ufxK6"
      },
      "source": [
        "# !unzip -uq '/content/drive/My Drive/Base.zip' -d '/content/drive/My Drive/Base'\n",
        "# fileName=os.listdir('/content/drive/My Drive/Base/Base')\n",
        "# total=len(fileName)\n",
        "# f = open('file_list.txt','a')\n",
        "# for i in range(0,total):\n",
        "#   f.write(fileName[i])\n",
        "#   f.write(\" \")\n",
        "# f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cnO7L9HU_lM"
      },
      "source": [
        "\n",
        "my_whole_seed = 111\n",
        "torch.manual_seed(my_whole_seed)\n",
        "torch.cuda.manual_seed_all(my_whole_seed)\n",
        "torch.cuda.manual_seed(my_whole_seed)\n",
        "np.random.seed(my_whole_seed)\n",
        "random.seed(my_whole_seed)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "    if name.islower() and not name.startswith(\"__\")\n",
        "    and callable(models.__dict__[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbMXLkRL_5Gf"
      },
      "source": [
        "Class AverageMeter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0WgH0eh_4h8"
      },
      "source": [
        "class AverageMeter(object):\n",
        "\n",
        "  \"\"\"Computes and stores the average and current value\"\"\"\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.val = 0\n",
        "    self.avg = 0\n",
        "    self.sum = 0\n",
        "    self.count = 0\n",
        "\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjMiAktjlV7P"
      },
      "source": [
        "class identifyDataset(data.Dataset):\r\n",
        "  def __init__(self,imageName,transform):\r\n",
        "\r\n",
        "    self.imageName = imageName\r\n",
        "    self.transform = transform\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.imageName)\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    img = Image.open(self.imageName)\r\n",
        "    image = img.convert('RGB')\r\n",
        "    image = self.transform(image)\r\n",
        "    return image\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjGdDVwK8G4I"
      },
      "source": [
        "Class CANet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlp5dPSrd7Oz"
      },
      "source": [
        "class CANet:\n",
        "  def __init__(self):\n",
        "\n",
        "    self.data = '/content/drive/My Drive/Base/Base/' #path to dataset\n",
        "    self.dataset = \"missidor\"\n",
        "    self.model_dir = '/content/drive/My Drive/Trained/'\n",
        "    self.a = 'resnet50'\n",
        "    self.arch = 'resnet50' #default 'alexnet'\n",
        "    self.j = 4\n",
        "    self.workers = 4\n",
        "    self.epochs = 1000 #default 300\n",
        "    self.num_class = 2\n",
        "    self.start_epoch = 0\n",
        "    self.batch_size = 40 #default 20\n",
        "    self.lr = 0.1\n",
        "    self.learning_rate = 0.1\n",
        "    self.momentum = 0.9\n",
        "    self.wd = 1e-4\n",
        "    self.liu = False\n",
        "    self.weight_decay = 1e-4\n",
        "    self.p = 10\n",
        "    self.print_freq = 10\n",
        "    #e , evaluate boolean evaluates model on validation set\n",
        "    self.resume=''\n",
        "    self.e = False\n",
        "    self.evaluate = False\n",
        "    self.pretrained = True #boolean uses pretrained model\n",
        "    self.decay_epoch = 500 #default 15\n",
        "    self.seed = 111 #for initializing training\n",
        "    self.gpu = 0 #GPU id to use\n",
        "    #multitask , liu , chen , crossCBAM , adam , choice stores boolean\n",
        "    self.adam = True\n",
        "    self.crossCBAM = True\n",
        "    self.multitask = True\n",
        "    self.chen = False\n",
        "    self.crosspatialCBAM = False\n",
        "    self.CAN_TS = False\n",
        "    self.choice = \"both\"\n",
        "    self.net_type = \"regular\"\n",
        "    self.channels = 109\n",
        "    self.nodes = 32\n",
        "    self.graph_model = \"WS\"\n",
        "    self.K = 4\n",
        "    self.P = 0.75\n",
        "    self.fold_name = \"fold0\"\n",
        "    self.fold_name2 = \"fold2\"\n",
        "    self.fold_name3 = \"fold3\"\n",
        "    #lr.......\n",
        "    self.lr_mode = \"cosine\"\n",
        "    self.base_lr = 3e-4 #default 0.03\n",
        "    self.warmup_epochs = 0\n",
        "    self.warmup_lr = 0.0\n",
        "    self.targetlr = 0.0\n",
        "    self.lambda_value = 0.25\n",
        "    self.best_acc1 = 0\n",
        "    self.best_auc = 0\n",
        "    self.best_accdr = 0\n",
        "    self.minimum_loss = 1\n",
        "    self.count = 0\n",
        "    pass\n",
        "\n",
        "  def count_parameters(self,model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    pass\n",
        "\n",
        "  def worker_init_fn(self,worker_id):\n",
        "    random.seed(1 + worker_id)\n",
        "    pass  \n",
        "    \n",
        "  def softmax(self,x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "  # def save_checkpoint(self,state, is_best, filename='checkpoint.pth', save_dir= 'file'):\n",
        "    \n",
        "  #   root = save_dir + \"/\"\n",
        "  #   if not os.path.exists(root):\n",
        "  #       os.makedirs(root)\n",
        "  #   torch.save(state, root+filename)\n",
        "  #   pass\n",
        "\n",
        "  # def save_result2txt(self,savedir, all_output_dme, all_output,all_target_dme,all_target):\n",
        "\n",
        "  #   np.savetxt(savedir+\"/output_dme.txt\", all_output_dme, fmt='%.4f')\n",
        "  #   np.savetxt(savedir+\"/output_dr.txt\", all_output, fmt='%.4f')\n",
        "  #   np.savetxt(savedir+\"/target_dme.txt\", all_target_dme)\n",
        "  #   np.savetxt(savedir+\"/target_dr.txt\", all_target)\n",
        "  #   pass\n",
        "  \n",
        "  def multi_class_auc(self,all_target, all_output, num_c = None):\n",
        "\n",
        "    # multi-class accuraccy\n",
        "    all_output = np.stack(all_output)\n",
        "    all_target = label_binarize(all_target, classes=list(range(0, num_c)))\n",
        "    auc_sum = []\n",
        "    for num_class in range(0, num_c):\n",
        "      try:\n",
        "        auc = roc_auc_score(all_target[:, num_class], all_output[:, num_class])\n",
        "        auc_sum.append(auc)\n",
        "      except ValueError:\n",
        "        pass\n",
        "\n",
        "    auc = sum(auc_sum) / float(len(auc_sum))\n",
        "    return auc\n",
        "\n",
        "  def save_result_txt(self,savedir, result):\n",
        "\n",
        "    with open(savedir + '/result1(1).txt', 'w') as f:\n",
        "      for item in result:\n",
        "        f.write(\"%.8f\\n\" % item)\n",
        "      f.close()\n",
        "\n",
        "  def train(self,train_loader, model, criterion, lr_scheduler, writer, epoch, optimizer):\n",
        "    \n",
        "    print(\"Training...\")\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target, name) in enumerate(train_loader):\n",
        "      # measure data loading time\n",
        "      data_time.update(time.time() - end)\n",
        "      lr = lr_scheduler.update(i, epoch)\n",
        "      writer.add_scalar(\"lr\", lr, epoch)\n",
        "      if   self.gpu is not None:\n",
        "        input = input.cuda(  self.gpu, non_blocking = True)\n",
        "      if   self.multitask:\n",
        "        target = [item.cuda(  self.gpu, non_blocking = True) for item in target]\n",
        "      else:\n",
        "        target = target.cuda(  self.gpu, non_blocking = True)\n",
        "\n",
        "      # compute output\n",
        "      output = model(input)\n",
        "      if   self.multitask:\n",
        "        loss1 = criterion(output[0], target[0])\n",
        "        loss2 = criterion(output[1], target[1])\n",
        "        if   self.crossCBAM:\n",
        "          loss3 = criterion(output[2], target[0])\n",
        "          loss4 = criterion(output[3], target[1])\n",
        "          loss = (loss1 + loss2 +   self.lambda_value *loss3 +   self.lambda_value * loss4)\n",
        "        else:\n",
        "          loss = (loss1 + loss2)\n",
        "      else:\n",
        "        loss = criterion(output, target)\n",
        "      \n",
        "      losses.update(loss.item(), input.size(0))\n",
        "\n",
        "      # compute gradient and do SGD step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # measure elapsed time\n",
        "      batch_time.update(time.time() - end)\n",
        "      end = time.time()\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "  def validate(self,val_loader, model):\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    print(\"Validating...\")\n",
        "    model.eval()\n",
        "    all_target = []\n",
        "    all_target_dme = []\n",
        "    all_output = []\n",
        "    all_name = []\n",
        "    all_output_dme = []\n",
        "    # writer = SummaryWriter(self.model_dir)\n",
        "    # writer.add_text('Text', \"Comparision of labels\")\n",
        "    # writer.add_text(\"Text\", \"Predicted on axis-y\")\n",
        "    # writer.add_text(\"Text\", \"Truth on axis-x\")\n",
        "    with torch.no_grad():\n",
        "      for i, (input, target, name) in enumerate(val_loader):\n",
        "        if   self.gpu is not None:\n",
        "          input = input.cuda(  self.gpu, non_blocking=True)\n",
        "        if   self.multitask:\n",
        "          target = [item.cuda(  self.gpu, non_blocking=True) for item in target]\n",
        "        else:\n",
        "          target = target.cuda(  self.gpu, non_blocking=True)\n",
        "        print(\"Image Shape:  \",input.shape)\n",
        "        output = model(input)\n",
        "        torch.cuda.synchronize()\n",
        "        if   self.multitask:\n",
        "          output0 = output[0] #dr\n",
        "          output1 = output[1] #dme\n",
        "          output0 = torch.softmax(output0, dim=1)\n",
        "          output1 = torch.softmax(output1, dim=1)\n",
        "          all_target.append(target[0].cpu().data.numpy())\n",
        "          all_output.append(output0.cpu().data.numpy())\n",
        "          all_target_dme.append(target[1].cpu().data.numpy())\n",
        "          all_output_dme.append(output1.cpu().data.numpy())\n",
        "        else:\n",
        "          output = torch.softmax(output, dim=1)\n",
        "          all_target.append(target.cpu().data.numpy())\n",
        "          all_output.append(output.cpu().data.numpy())\n",
        "        all_name.append(name)\n",
        "    \n",
        "    if   self.dataset == \"kaggle\":\n",
        "      all_output = [item for sublist in all_output for item in sublist]\n",
        "      all_target = [item for sublist in all_target for item in sublist]\n",
        "      acc_dr = accuracy_score(all_target, np.argmax(all_output, axis=1))\n",
        "      auc_dr = multi_class_auc(all_target, all_output, num_c=5)\n",
        "      return acc_dr, auc_dr\n",
        "    elif not   self.multitask:\n",
        "      all_target = [item for sublist in all_target for item in sublist]\n",
        "      all_output = [item for sublist in all_output for item in sublist]\n",
        "      if   self.num_class == 2:\n",
        "        acc = accuracy_score(all_target, np.argmax(all_output,axis=1))\n",
        "        auc = roc_auc_score(all_target, [item[1] for item in all_output])\n",
        "        precision_dr = precision_score(all_target, np.argmax(all_output, axis=1))\n",
        "        recall_dr = recall_score(all_target, np.argmax(all_output, axis=1))\n",
        "        f1score_dr = f1_score(all_target, np.argmax(all_output, axis=1))\n",
        "      else:\n",
        "        acc = accuracy_score(all_target, np.argmax(all_output, axis=1))\n",
        "        auc = self.multi_class_auc(all_target, all_output, num_c=3)\n",
        "        precision_dr = precision_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "        recall_dr = recall_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "        f1score_dr = f1_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "      return acc, auc, precision_dr, recall_dr, f1score_dr\n",
        "    else:\n",
        "      all_target = [item for sublist in all_target for item in sublist]\n",
        "      all_output = [item for sublist in all_output for item in sublist]\n",
        "      all_target_dme = [item for sublist in all_target_dme for item in sublist]\n",
        "      all_output_dme = [item for sublist in all_output_dme for item in sublist]\n",
        "      # acc\n",
        "      acc_dr = accuracy_score(all_target, np.argmax(all_output,axis=1)) #1st arg predicted label\n",
        "      acc_dme = accuracy_score(all_target_dme, np.argmax(all_output_dme, axis=1))\n",
        "      # pr_dme = np.argmax(all_output_dme,axis=1)\n",
        "      # tr_dme = np.argmax(all_target_dme)\n",
        "      # writer.add_scalar('Label DME', pr_dme[0], tr_dme)\n",
        "    \n",
        "      # joint acc\n",
        "      joint_result = np.vstack((np.argmax(all_output, axis=1), np.argmax(all_output_dme, axis=1)))\n",
        "      joint_target = np.vstack((all_target, all_target_dme))\n",
        "      joint_acc = ((np.equal(joint_result, joint_target) == True).sum(axis=0) == 2).sum() / joint_result.shape[1]\n",
        "      # auc\n",
        "      if   self.dataset == \"missidor\":\n",
        "        auc_dr  = roc_auc_score(all_target, [item[1] for item in all_output])\n",
        "      else:\n",
        "        auc_dr = self.multi_class_auc(all_target, all_output, num_c = 5)\n",
        "      auc_dme = self.multi_class_auc(all_target_dme, all_output_dme, num_c = 3)\n",
        "      # precision\n",
        "      if   self.dataset == \"missidor\":\n",
        "        precision_dr = precision_score(all_target, np.argmax(all_output, axis=1))\n",
        "      else:\n",
        "        precision_dr = precision_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "      precision_dme = precision_score(all_target_dme, np.argmax(all_output_dme, axis=1), average=\"macro\")\n",
        "      # recall\n",
        "      if   self.dataset == \"missidor\":\n",
        "        recall_dr = recall_score(all_target, np.argmax(all_output, axis=1))\n",
        "      else:\n",
        "        recall_dr  = recall_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "      recall_dme = recall_score(all_target_dme, np.argmax(all_output_dme, axis=1), average=\"macro\")\n",
        "      # f1_score\n",
        "      if   self.dataset == \"missidor\":\n",
        "        f1score_dr = f1_score(all_target, np.argmax(all_output, axis=1))\n",
        "      else:\n",
        "        f1score_dr = f1_score(all_target, np.argmax(all_output, axis=1), average=\"macro\")\n",
        "      f1score_dme = f1_score(all_target_dme, np.argmax(all_output_dme, axis=1), average=\"macro\")\n",
        "\n",
        "      cm1 = confusion_matrix(all_target, np.argmax(all_output, axis=1))\n",
        "      sensitivity1 = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n",
        "      specificity1 = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n",
        "      \n",
        "      # writer.close()\n",
        "      return acc_dr, acc_dme, joint_acc, \\\n",
        "            [auc_dr, auc_dme, precision_dr, precision_dme, recall_dr, recall_dme, f1score_dr, f1score_dme],\\\n",
        "            sensitivity1, specificity1\n",
        "\n",
        "  def main_worker(self):\n",
        "\n",
        "    if not os.path.exists(  self.model_dir):\n",
        "      os.makedirs(  self.model_dir)\n",
        "\n",
        "    if   self.gpu is not None:\n",
        "      print(\"Use GPU: {} for training\".format(  self.gpu))\n",
        "\n",
        "    if   self.arch == \"resnet50\":\n",
        "      #from models.resnet50 import resnet50\n",
        "      model = resnet50.resnet50(num_classes=  self.num_class, multitask=  self.multitask, liu=  self.liu,\n",
        "                chen=  self.chen, CAN_TS=  self.CAN_TS, crossCBAM=  self.crossCBAM,\n",
        "                crosspatialCBAM =   self.crosspatialCBAM,  choice=  self.choice)\n",
        "\n",
        "    if   self.pretrained:\n",
        "      model_dict = model.state_dict()\n",
        "      pretrain_path = {\"resnet50\": \"/content/drive/My Drive/resnet50-19c8e357.pth\",}[  self.arch]\n",
        "      pretrained_dict = torch.load(pretrain_path)\n",
        "      pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "      pretrained_dict.pop('classifier.weight', None)\n",
        "      pretrained_dict.pop('classifier.bias', None)\n",
        "      model_dict.update(pretrained_dict)\n",
        "      model.load_state_dict(model_dict)\n",
        "\n",
        "    torch.cuda.set_device(  self.gpu)\n",
        "    model = model.cuda(  self.gpu)\n",
        "\n",
        "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda(  self.gpu)\n",
        "    if   self.adam:\n",
        "      optimizer = torch.optim.Adam(model.parameters(),   self.base_lr, weight_decay=  self.weight_decay)\n",
        "    else:\n",
        "      optimizer = torch.optim.SGD(model.parameters(),   self.base_lr,\n",
        "                              momentum=  self.momentum,\n",
        "                              weight_decay=  self.weight_decay)\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    size  = 224\n",
        "    tra = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomResizedCrop(size),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                # transforms.RandomRotation(90),\n",
        "                # transforms.ColorJitter(0.05, 0.05, 0.05, 0.05),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "    \n",
        "    train_dataset = missidor.traindataset(root=  self.data, mode='train', transform=tra, num_class=  self.num_class,\n",
        "                                multitask=  self.multitask,   args =  self)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=  self.batch_size, shuffle=True,\n",
        "        num_workers=  self.workers, pin_memory=True,worker_init_fn=self.worker_init_fn)\n",
        "\n",
        "    writer = SummaryWriter(self.model_dir + \"runs\")\n",
        "    writer.add_text('Text', \"CANet\")\n",
        "    #\n",
        "    #from lr_scheduler import LRScheduler\n",
        "    lr_scheduler = l_rate.LRScheduler(optimizer, len(train_loader),self)\n",
        "\n",
        "    for epoch in range(  self.start_epoch,   self.epochs):\n",
        "      is_best = False\n",
        "      is_best_auc = False\n",
        "      is_best_acc = False\n",
        "      # train for one epoch\n",
        "      loss_train = self.train(train_loader, model, criterion, lr_scheduler, writer, epoch, optimizer)\n",
        "      template = 'Epoch {}, Loss: {}'\n",
        "      print (template.format(epoch+1,\n",
        "              loss_train))\n",
        "      writer.add_scalar('Train loss', loss_train, epoch)\n",
        "    writer.close()\n",
        "\n",
        "    torch.save({'epoch': self.epochs,\n",
        "                              'state_dict': model.state_dict(),\n",
        "                              'optim_dict' : optimizer.state_dict()}, self.model_dir + \"CANet(3).pth\",_use_new_zipfile_serialization=False)\n",
        "    \n",
        "  def EvaluateModel(self):\n",
        "    self.resume = '/content/drive/MyDrive/Trained/CANet(2).pth'\n",
        "    self.evaluate = True\n",
        "    self.e = True\n",
        "\n",
        "    if   self.arch == \"resnet50\":\n",
        "      #from models.resnet50 import resnet50\n",
        "      model = resnet50.resnet50(num_classes=  self.num_class, multitask=  self.multitask, liu=  self.liu,\n",
        "                chen=  self.chen, CAN_TS=  self.CAN_TS, crossCBAM=  self.crossCBAM,\n",
        "                crosspatialCBAM =   self.crosspatialCBAM,  choice=  self.choice)\n",
        "\n",
        "    if   self.pretrained:\n",
        "      \n",
        "      model_dict = model.state_dict()\n",
        "      pretrain_path = {\"resnet50\": \"/content/drive/My Drive/resnet50-19c8e357.pth\",}[  self.arch]\n",
        "      pretrained_dict = torch.load(pretrain_path)\n",
        "      pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "      pretrained_dict.pop('classifier.weight', None)\n",
        "      pretrained_dict.pop('classifier.bias', None)\n",
        "      model_dict.update(pretrained_dict)\n",
        "      model.load_state_dict(model_dict)\n",
        "\n",
        "    torch.cuda.set_device(  self.gpu)\n",
        "    model = model.cuda(  self.gpu)\n",
        "\n",
        "    if   self.resume:\n",
        "      if os.path.isfile(  self.resume):\n",
        "        print(\"=> loading checkpoint '{}'\".format(  self.resume))\n",
        "        checkpoint = torch.load(  self.resume)\n",
        "        #  load partial weights\n",
        "        if not   self.evaluate:\n",
        "          print (\"load partial weights\")\n",
        "          model_dict = model.state_dict()\n",
        "          pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict}\n",
        "          model_dict.update(pretrained_dict)\n",
        "          model.load_state_dict(model_dict)\n",
        "        else:\n",
        "          print(\"loading weights\")\n",
        "          model.load_state_dict(checkpoint['state_dict'])\n",
        "          # optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "      else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(  self.resume))\n",
        "        exit(0)\n",
        "    \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "    \n",
        "    tra_test = transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          normalize])\n",
        "    val_dataset = missidor.traindataset(root=  self.data, mode = 'val',\n",
        "                            transform=tra_test, num_class=  self.num_class,\n",
        "                            multitask=  self.multitask,   args =  self)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "      val_dataset,\n",
        "      batch_size=  self.batch_size, shuffle=False,\n",
        "      num_workers=  self.workers, pin_memory=True)\n",
        "    \n",
        "    if   self.evaluate:\n",
        "      a = time.time()\n",
        "      savedir = self.model_dir\n",
        "      if not   self.multitask:\n",
        "        acc, auc, precision_dr, recall_dr, f1score_dr  = self.validate(val_loader, model)\n",
        "        result_list = [acc, auc, precision_dr, recall_dr, f1score_dr]\n",
        "        print (\"acc, auc, precision, recall, f1\", acc, auc, precision_dr, recall_dr, f1score_dr)\n",
        "        self.save_result_txt(savedir, result_list)\n",
        "        print(\"time\", time.time() - a)\n",
        "        return\n",
        "      else:\n",
        "        acc_dr, acc_dme, acc_joint, other_results, se, sp = self.validate(val_loader, model)\n",
        "        print (\"acc_dr, acc_dme, acc_joint\", acc_dr, acc_dme, acc_joint)\n",
        "        print (\"auc_dr, auc_dme, precision_dr, precision_dme, recall_dr, recall_dme, f1score_dr, f1score_dme\",\n",
        "                other_results)\n",
        "        print (\"se, sp\", se, sp)\n",
        "        result_list = [acc_dr, acc_dme, acc_joint]\n",
        "        result_list += other_results\n",
        "        result_list += [se, sp]\n",
        "        self.save_result_txt(savedir, result_list)\n",
        "\n",
        "        print (\"time\", time.time()-a)\n",
        "        return\n",
        "    \n",
        "  def identify(self,name):\n",
        "    all_output = []\n",
        "    all_output_dme = []\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    tra_test = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize])\n",
        "    \n",
        "    path = '/content/drive/My Drive/' + name\n",
        "    imageTra = identifyDataset(imageName=path,transform=tra_test)\n",
        "    img_loader = torch.utils.data.DataLoader(\n",
        "        imageTra,\n",
        "        batch_size=  1, shuffle=False,\n",
        "        num_workers=  0, pin_memory=True)\n",
        "    \n",
        "    model = resnet50.resnet50(num_classes=  2, multitask=  False, liu=  False,\n",
        "                chen=  False, CAN_TS=  False, crossCBAM=  True,\n",
        "                crosspatialCBAM =   False,  choice=  False)\n",
        "    \n",
        "    model_dict = model.state_dict()\n",
        "    pretrain_path = {\"resnet50\": \"/content/drive/My Drive/resnet50-19c8e357.pth\",}[  self.arch]\n",
        "    pretrained_dict = torch.load(pretrain_path)\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    pretrained_dict.pop('classifier.weight', None)\n",
        "    pretrained_dict.pop('classifier.bias', None)\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "    CUDA_VISIBLE_DEVICES=\"\"\n",
        "    # torch.cuda.set_device(  self.gpu)\n",
        "    # model = model.cuda(  self.gpu)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(),   self.base_lr, weight_decay=  self.weight_decay)\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/Trained/CANet(2).pth',map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "    print(\"model loaded\")\n",
        "    model.eval()\n",
        "    model.to('cpu')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      # for i, (input) in enumerate(img_loader):\n",
        "      #   print(i)\n",
        "\n",
        "        # if  self.gpu is not None:\n",
        "        #   input = input.cuda(  self.gpu, non_blocking=True)\n",
        "\n",
        "      data_iter=iter(img_loader)\n",
        "      input = data_iter.next()\n",
        "      output = model(input)\n",
        "      # torch.cuda.synchronize()\n",
        "      output0 = output[0] #dr\n",
        "      output1 = output[1] #dme\n",
        "      output0 = torch.softmax(output0, dim=1)\n",
        "      output1 = torch.softmax(output1, dim=1)\n",
        "      all_output.append(output0.cpu().data.numpy())\n",
        "      all_output_dme.append(output1.cpu().data.numpy())\n",
        "      all_output = [item for sublist in all_output for item in sublist]\n",
        "      all_output_dme = [item for sublist in all_output_dme for item in sublist]\n",
        "      pr_dr = np.argmax(all_output,axis=1)\n",
        "      pr_dme = np.argmax(all_output_dme,axis=1)\n",
        "\n",
        "      print(\"Name: \", name ,\"DR_stage : \", pr_dr[0])\n",
        "      print(\"Name: \", name ,\"DME_stage : \", pr_dme[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZsC8IQUNdef"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTw251wg6S2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d810b9a-f314-4d2e-8f5c-0494ad513928"
      },
      "source": [
        "# 70 30 split\n",
        "#for dataset of 1200 ==> 360 test & 840 training\n",
        "#for dataset of 800 ==> 240 test & 560 training\n",
        "test = CANet()\n",
        "#for training------\n",
        "#test.main_worker() \n",
        "#for model validation------\n",
        "test.EvaluateModel()  \n",
        "#for subject image stage identification------\n",
        "# test.identify('IDRiD_006.jpg') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint '/content/drive/MyDrive/Trained/CANet(2).pth'\n",
            "loading weights\n",
            "=> Total Test:  240  Multi-Task images \n",
            "Validating...\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "Image Shape:   torch.Size([40, 3, 224, 224])\n",
            "acc_dr, acc_dme, acc_joint 0.8458333333333333 0.9 0.7833333333333333\n",
            "auc_dr, auc_dme, precision_dr, precision_dme, recall_dr, recall_dme, f1score_dr, f1score_dme [0.9066666666666666, 0.8829788843633278, 0.7676767676767676, 0.7402146464646465, 0.8444444444444444, 0.709198018476369, 0.8042328042328042, 0.7238129082478763]\n",
            "se, sp 0.8466666666666667 0.8444444444444444\n",
            "time 3.96866512298584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_vRIGzNo6j"
      },
      "source": [
        "For visual representation of Summary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_L42YG7MkKl"
      },
      "source": [
        "# %load_ext tensorboard\r\n",
        "# %tensorboard --logdir \"/content/drive/MyDrive/Trained\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLRsjownQRb7"
      },
      "source": [
        "# # !pip install Pillow==8.0.1\r\n",
        "# from PIL import Image\r\n",
        "# import glob\r\n",
        "# import xlrd\r\n",
        "# import numpy as np\r\n",
        "# import scipy.misc\r\n",
        "# import os\r\n",
        "# import os.path\r\n",
        "# import sys\r\n",
        "# if sys.version_info[0] == 2:\r\n",
        "#     import cPickle as pickle\r\n",
        "# else:\r\n",
        "#     import pickle\r\n",
        "# import torch.utils.data as data\r\n",
        "# import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpAz4iTCGijd"
      },
      "source": [
        "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n",
        "#                                      std=[0.229, 0.224, 0.225])\r\n",
        "# tra_test = transforms.Compose([\r\n",
        "#             transforms.Resize(256),\r\n",
        "#             transforms.CenterCrop(224),\r\n",
        "#             transforms.ToTensor(),\r\n",
        "#             normalize])\r\n",
        "# model = resnet50.resnet50(num_classes=  2, multitask=  True, liu=  False,\r\n",
        "#                 chen=  False, CAN_TS=  False, crossCBAM=  True,\r\n",
        "#                 crosspatialCBAM =   False,  choice=  False)\r\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Trained/CANet(1).pth'))\r\n",
        "# print(\"model loaded\")\r\n",
        "# model.eval()\r\n",
        "# %cd \"/content/drive/MyDrive/Base/Base/\"\r\n",
        "# img = np.array(Image.open(\"20051020_45110_0100_PP.tif\"))\r\n",
        "# img_cv = cv2.imread('20051020_45110_0100_PP.tif')\r\n",
        "# print(\"Image: \",img.size)\r\n",
        "# print(\"cv2 Image \",img_cv.shape)\r\n",
        "# im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
        "# cv2.imwrite('/content/drive/MyDrive/20051020_45110_0100_PP.tif',im_rgb)\r\n",
        "# # img = Image.open('/content/drive/MyDrive/20051020_45110_0100_PP.tif')\r\n",
        "# # image = img.convert('RGB')\r\n",
        "# # img.show()\r\n",
        "# img = Image.fromarray(im_rgb)\r\n",
        "# #img = img.convert(\"RGB\")\r\n",
        "# print(\"Image mode: \",img.mode)\r\n",
        "# print(\"Image: \",img.size)\r\n",
        "# red, green, blue = img.split()\r\n",
        "# print(\"r mode: \",red.mode)\r\n",
        "# print(\"g mode: \",green.mode)\r\n",
        "# print(\"b mode: \",blue.mode)\r\n",
        "# r, g, b = img.getpixel((1, 1))\r\n",
        "# print(r, g, b)\r\n",
        "# new_img = Image.merge(\"RGB\",(red,green,blue))\r\n",
        "# print(\"new size \" ,new_img.size)\r\n",
        "# imageTra = tra_test(new_img)\r\n",
        "# img_loader = torch.utils.data.DataLoader(\r\n",
        "#         imageTra,\r\n",
        "#         batch_size=  1, shuffle=False,\r\n",
        "#         num_workers=  4, pin_memory=True)\r\n",
        "# dataiter = iter(img_loader)\r\n",
        "# image= dataiter.next()\r\n",
        "# print(\"Image shape:  \",image.shape)\r\n",
        "# output = model(image)\r\n",
        "# acc_dr = output[0]\r\n",
        "# acc_dme = output[1]\r\n",
        "# print(\"DR_stage : \", acc_dr)\r\n",
        "# print(\"DME_stage : \", acc_dme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y-d-KyeBt3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}